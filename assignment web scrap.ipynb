{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7fd185-3f8d-4e9c-8119-a68f09c4b9ad",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9b44f-1880-4b0f-9519-cff2d00d6e00",
   "metadata": {},
   "source": [
    "web scraping is is automatic method to obtain large amount of data from the website . and most of the data is wriiten in unstructured way which convert into sturctured format in a data base and used in various application . using in online service specially in API to obtain data from website\n",
    "in general websracping consist the two parts nameli crawler and scraper ,crawler is used for AI algorithm and scraper is used to extract the data from website\n",
    "web scraping is is generally used by industry to get or extract the data from website some of the data are written below-\n",
    "1 price monitoring - it is used by the compnies to monitoring the price of the product compare the price with others\n",
    "2 market research- it is used to analyze the trends of customers like whats they demand in the current cenario\n",
    "3 email marketing - by the use of web scraping we can get the email of customers and sending the emails to them for the promotions and other purposes  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db1e68-0ead-4098-b962-43cba0502479",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bf5f2-4af7-4c68-b28e-863fbdb6c3c9",
   "metadata": {},
   "source": [
    "beautiful soup are using while we scrap the data and function to beautify the complex data . in general it is a python library that makes  easy to scrap the information from web page wheather it in the form of exl,ot html or other mark up language \n",
    "Beutiful soup automatically convert the incoming document to unicode and outgoing document to utf-8\n",
    "\n",
    "advantage\n",
    "1 beautiful soup is user friendly\n",
    "2 it does not require the browser instance\n",
    "3 It's faster\n",
    "4 It's beginner-friendly and easier to set up\n",
    "5 It works independently from browsers\n",
    "6 It requires less time to run\n",
    "7 It can parse HTML and XML documents\n",
    "8 BeautifulSoup is easier to debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae064f-ca10-49e7-9dda-51496ce6dc69",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3e63e-3de6-4204-abfb-31044645a22c",
   "metadata": {},
   "source": [
    "lask is considered a micro web framework is written in Python programming. The word “micro” means focusing to keep the core extensible but straightforward. It is not dependent upon external libraries to perform the tasks of a framework. Flask framework is more independent, flexible, and simple; so many developers prefer to start with Flask.\n",
    "1 It is a lightweight framework that offers hassle-free development\n",
    "2 Provide flexibility to the developer to experiment with their modules or architecture\n",
    "3 It is suitable for small projects\n",
    "4 Offers a built-in development server and fast debugger\n",
    "5 Easily scalable for the applications\n",
    "6 Support for secure cookies\n",
    "7 Uses Ninja2 Template engine\n",
    "8 It has integrated support for unit testing\n",
    "9 APIs are coherent and neat\n",
    "10 Provide strong WSGI support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86572e24-0a35-4883-b96d-1e4cb23d07a4",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb238485-210e-4cb3-81b2-0379a10add00",
   "metadata": {},
   "source": [
    "diffrent method used for web scraping\n",
    "1 Human copy-and-paste.\n",
    "2 Text pattern matching.\n",
    "3 HTTP programming.\n",
    "4 HTML parsing.\n",
    "\n",
    "1 Human copy-and-paste-Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping.\n",
    "\n",
    "2  Text pattern matching-he UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "HTTP Programming\n",
    "\n",
    "4 HTML parsing-Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60991fb7-e0bc-4b90-bde7-e03fd0bfe11b",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172c049-ca21-40e5-b1f5-cfab2670d428",
   "metadata": {},
   "source": [
    "1 AWS Elastic Beanstalk is an AWS-managed service for web applications. Elastic Beanstalk is a pre-configured EC2 server that can directly take up your application code and environment configurations and use it to automatically provision and deploy the required resources within AWS to run the web application. Unlike EC2 which is Infrastructure as a service, Elastic Beanstalk is a Platform As A Service (PAAS) as it allows users to directly use a pre-configured server for their application. Of course, you can deploy applications without ever having to use elastic beanstalk but that would mean having to choose the appropriate service from the vast array of services offered by  AWS, manually provisioning these AWS resources, and stitching them up together to form a complete web application. Elastic Beanstalk abstracts the underlying configuration work and allows you as a user to focus on more pressing matters.\n",
    "\n",
    "2 AWS Code Pipeline is a fully managed continuous delivery service that helps you automate your release pipeline. It allows users to build, test and deploy code into a test or production environment using either the AWS CLI or a clean UI configuration process within the Amazon Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c2055-ad83-4ddb-8bd1-c7a23f93dfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f4df5-18be-4606-9031-cd83e659a171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
